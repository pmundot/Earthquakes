{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Vibes: The Probability of Earthquake Magnitude and Death\n",
    "*A Milestone Report for the Introduction to Data Science Project*  \n",
    "  \n",
    "**Team Fugacity Members**  \n",
    "Paul Mundt, u0932146  \n",
    "Katie Jones, u0541901  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Cleaning\n",
    "\n",
    "The main source of data for the regressions comes from the Earthquake Impact Database on earthquake-report.com. For the analysis, we are analyzing data from 2017-2020. The data from 2020 is partial, and includes information that was availible on Wednesday, March 18th. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import scipy as sci\n",
    "from requests import get\n",
    "import folium\n",
    "from geopy.geocoders import ArcGIS\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as sm\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline  \n",
    "plt.rcParams['figure.figsize'] = (12, 12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CSV Files Extracted from the Earthquake Impact Database\n",
    "data_2017 = pd.read_csv(\"2017_raw.csv\", encoding = \"ISO-8859-1\").dropna(axis = 0, how = \"all\")\n",
    "data_2018 = pd.read_csv(\"2018_raw.csv\", encoding = \"ISO-8859-1\").dropna(axis = 0, how = \"all\")\n",
    "data_2019 = pd.read_csv(\"2019_raw.csv\", encoding = \"ISO-8859-1\").dropna(axis = 0, how = \"all\")\n",
    "data_2020 = pd.read_csv(\"2020_raw.csv\", encoding = \"ISO-8859-1\").dropna(axis = 0, how = \"all\", thresh = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean 2020 Data\n",
    "data_2020 = data_2020.drop('Depth (km)', axis = 1).drop('Type', axis = 1).drop('Origin', axis = 1).drop('Tsunami height', axis = 1)\n",
    "data_2020 = data_2020.fillna(value = 0)\n",
    "data_2020 = data_2020.rename(columns = {\"Epicenter\" : \"Country\", \"Region (Epicenter)\" : \"Region\"} )\n",
    "\n",
    "#Clean 2019 Data\n",
    "data_2019 = data_2019.drop('Depth (km)', axis = 1).drop('Type', axis = 1).drop('Origin', axis = 1).drop('Tsunami height', axis = 1)\n",
    "data_2019 = data_2019.fillna(value = 0).replace(\"--\", \"0\")\n",
    "data_2019 = data_2019.rename(columns = {\"Country (Epicenter)\" : \"Country\", \"Region (Epicenter)\" : \"Region\"} )\n",
    "\n",
    "#Clean 2018 Data\n",
    "data_2018 = data_2018.drop('Depth (km)', axis = 1).drop('Type', axis = 1).drop('Origin', axis = 1).drop('Tsunami height', axis = 1)\n",
    "data_2018 = data_2018.fillna(value = 0).replace(\"--\", \"0\")\n",
    "latlong_2018 = [0 for _ in range(0, len(data_2018.index))]\n",
    "data_2018.insert(loc = 4, column = 'Lat', value = latlong_2018)\n",
    "data_2018.insert(loc = 5, column = 'Long', value = latlong_2018)\n",
    "data_2018 = data_2018.rename(columns = {\"Country (Epicenter)\" : \"Country\", \"Region (Epicenter)\" : \"Region\"} )\n",
    "\n",
    "#Clean 2017 Data\n",
    "data_2017 = data_2017.drop('Depth (km)', axis = 1).drop('Tsunami height', axis = 1)\n",
    "data_2017 = data_2017.fillna(value = 0).replace(\"--\", \"0\")\n",
    "latlong_2017 = [0 for _ in range(0, len(data_2017.index))]\n",
    "data_2017.insert(loc = 4, column = 'Lat', value = latlong_2017)\n",
    "data_2017.insert(loc = 5, column = 'Long', value = latlong_2017)\n",
    "data_2017 = data_2017.rename(columns = {\"Impact coefficient (D)\" : \"Impact value (D)\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate Dataframes into a Single Dataset\n",
    "earthquake_data = pd.concat([data_2017, data_2018, data_2019, data_2020], sort = False).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_data['Intensity (MMI / JMA)']=earthquake_data['Intensity (MMI / JMA)'].str.replace('+',' ')\n",
    "earthquake_data['Intensity (MMI / JMA)']=earthquake_data['Intensity (MMI / JMA)'].str.replace('-',' ')\n",
    "earthquake_data['Intensity (MMI / JMA)']=earthquake_data['Intensity (MMI / JMA)'].str.replace('JMA',' ')\n",
    "earthquake_data['Intensity (MMI / JMA)']=earthquake_data['Intensity (MMI / JMA)'].str.replace('/ 7',' ')\n",
    "earthquake_data['Intensity (MMI / JMA)']=earthquake_data['Intensity (MMI / JMA)'].str.replace('Shindo ','')\n",
    "\n",
    "def roman_to_number(num):\n",
    "    rom_val = {'I': 1, 'V': 5, 'X': 10, 'L':50 , 'C':100, 'D':500, 'M':1000}\n",
    "    res=0\n",
    "    i=0\n",
    "    while i<len(num):\n",
    "        s1=num[i]\n",
    "        num1=rom_val[s1]\n",
    "        if i+1<len(num):\n",
    "            s2=num[i+1]\n",
    "            num2=rom_val[s2]\n",
    "            if num2<=num1:\n",
    "                res=res+num1\n",
    "                i+=1\n",
    "            else:\n",
    "                res=res+num2-num1\n",
    "                i+=2\n",
    "        else:\n",
    "            res=res+num1\n",
    "            i+=1\n",
    "    return(res)\n",
    "        \n",
    "    \n",
    "           \n",
    "def change(num):\n",
    "    if type(num)==str:\n",
    "        if len(num)>4:\n",
    "            tip=num[0:len(num)//2].strip()\n",
    "        else:\n",
    "            tip=num.strip()\n",
    "        try:\n",
    "            return(int(tip))\n",
    "        except:\n",
    "            return(roman_to_number(tip))\n",
    "    else:\n",
    "        return(num)\n",
    "earthquake_data['Intensity (MMI / JMA)']=[change(chest) for chest in earthquake_data['Intensity (MMI / JMA)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date (UTC)</th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Intensity (MMI / JMA)</th>\n",
       "      <th>Fatalities</th>\n",
       "      <th>Injuries</th>\n",
       "      <th>displaced</th>\n",
       "      <th>Impact value (D)</th>\n",
       "      <th>buildings damaged</th>\n",
       "      <th>buildings destroyed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1/2/2017</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Umbria</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.194401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1/3/2017</td>\n",
       "      <td>India</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1.668774</td>\n",
       "      <td>1456.0</td>\n",
       "      <td>166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1/3/2017</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Maranhao</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.752967</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1/3/2017</td>\n",
       "      <td>Fiji</td>\n",
       "      <td>Western Division (OS)</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1/6/2017</td>\n",
       "      <td>Iran</td>\n",
       "      <td>Fars</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>905.0</td>\n",
       "      <td>1.447852</td>\n",
       "      <td>400.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>954</td>\n",
       "      <td>3/13/2020</td>\n",
       "      <td>India</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>2.8</td>\n",
       "      <td>9.79</td>\n",
       "      <td>76.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038254</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>955</td>\n",
       "      <td>3/13/2020</td>\n",
       "      <td>Tanzania</td>\n",
       "      <td>Tanga</td>\n",
       "      <td>4.8</td>\n",
       "      <td>-4.90</td>\n",
       "      <td>38.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007925</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>956</td>\n",
       "      <td>3/14/2020</td>\n",
       "      <td>Iran</td>\n",
       "      <td>Qom</td>\n",
       "      <td>4.0</td>\n",
       "      <td>34.56</td>\n",
       "      <td>50.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038254</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>957</td>\n",
       "      <td>3/15/2020</td>\n",
       "      <td>Iran</td>\n",
       "      <td>Hormozgan</td>\n",
       "      <td>5.4</td>\n",
       "      <td>27.20</td>\n",
       "      <td>55.32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.235972</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>958</td>\n",
       "      <td>3/16/2020</td>\n",
       "      <td>India</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>3.1</td>\n",
       "      <td>26.90</td>\n",
       "      <td>75.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007925</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>959 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Date (UTC)   Country                 Region  Magnitude    Lat   Long  \\\n",
       "0     1/2/2017     Italy                 Umbria        4.1   0.00   0.00   \n",
       "1     1/3/2017     India                Tripura        5.5   0.00   0.00   \n",
       "2     1/3/2017    Brazil               Maranhao        4.6   0.00   0.00   \n",
       "3     1/3/2017      Fiji  Western Division (OS)        7.2   0.00   0.00   \n",
       "4     1/6/2017      Iran                   Fars        5.1   0.00   0.00   \n",
       "..         ...       ...                    ...        ...    ...    ...   \n",
       "954  3/13/2020     India                 Kerala        2.8   9.79  76.88   \n",
       "955  3/13/2020  Tanzania                  Tanga        4.8  -4.90  38.55   \n",
       "956  3/14/2020      Iran                    Qom        4.0  34.56  50.72   \n",
       "957  3/15/2020      Iran              Hormozgan        5.4  27.20  55.32   \n",
       "958  3/16/2020     India              Rajasthan        3.1  26.90  75.30   \n",
       "\n",
       "     Intensity (MMI / JMA)  Fatalities  Injuries  displaced  Impact value (D)  \\\n",
       "0                      5.0         0.0       0.0       30.0          0.194401   \n",
       "1                      NaN         3.0      49.0      600.0          1.668774   \n",
       "2                      NaN         0.0       0.0        0.0          0.752967   \n",
       "3                      4.0         0.0       0.0        0.0          0.000000   \n",
       "4                      NaN         4.0       4.0      905.0          1.447852   \n",
       "..                     ...         ...       ...        ...               ...   \n",
       "954                    NaN         0.0       0.0        0.0          0.038254   \n",
       "955                    NaN         0.0       0.0        0.0          0.007925   \n",
       "956                    NaN         0.0       0.0        0.0          0.038254   \n",
       "957                    NaN         0.0       2.0        0.0          0.235972   \n",
       "958                    NaN         0.0       0.0        0.0          0.007925   \n",
       "\n",
       "     buildings damaged  buildings destroyed  \n",
       "0                  0.0                  0.0  \n",
       "1               1456.0                166.0  \n",
       "2                500.0                  0.0  \n",
       "3                  0.0                  0.0  \n",
       "4                400.0                 40.0  \n",
       "..                 ...                  ...  \n",
       "954               10.0                  0.0  \n",
       "955                2.0                  0.0  \n",
       "956               10.0                  0.0  \n",
       "957               20.0                  1.0  \n",
       "958                2.0                  0.0  \n",
       "\n",
       "[959 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earthquake_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also able to get the current earthquake data from the USGS website and apply it to a webmap, with pop-ups. These pop-ups will hold future information for probability and other needed info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://feed.unmung.com/feed?feed=http://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/4.5+_month.atom'\n",
    "response=get(url)\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "Location=soup.find_all('a',class_=\"u-url\")\n",
    "Data=soup.find_all('span',class_=\"e-summary\")\n",
    "Location.pop(0)\n",
    "def mag_title(x):\n",
    "    magnitude=float(x[1:5])\n",
    "    location=x[7:]\n",
    "    return(magnitude,location)\n",
    "\n",
    "Locate=[]\n",
    "magni=[]\n",
    "for kip in Location:\n",
    "    inter=kip.get_text()\n",
    "    poper=mag_title(inter)\n",
    "    Locate.append(poper[1])\n",
    "    magni.append(poper[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'latitudel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-1c903a6037f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msets\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoord_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mlatitude\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatitudel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mlongitude\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlongitudel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'latitudel' is not defined"
     ]
    }
   ],
   "source": [
    "time=[]\n",
    "loc=[]\n",
    "depth=[]\n",
    "for stuff in Data:\n",
    "    j=stuff.find_all('dd')\n",
    "    time.append(j[0].get_text())\n",
    "    loc.append(j[2].get_text())\n",
    "    depth.append(j[3].get_text())   \n",
    "\n",
    "def lattitudel(x):\n",
    "    if \"S\" in x:\n",
    "        cord=-float(x.replace('°S',''))\n",
    "    else:\n",
    "        cord=float(x.replace('°N',''))\n",
    "    return(cord)\n",
    "\n",
    "def longitudel(x):\n",
    "    if \"W\" in x:\n",
    "        cord=-float(x.replace('°W',''))\n",
    "    else:\n",
    "        cord=float(x.replace('°E',''))\n",
    "    return(cord)\n",
    "def coord_split(x):\n",
    "    half=len(x)//2\n",
    "    first=x[:half]\n",
    "    last=x[half:]\n",
    "    return(first,last)\n",
    "\n",
    "latitude=[]\n",
    "longitude=[]\n",
    "for sets in loc:\n",
    "    x,y=coord_split(sets)\n",
    "    latitude.append(latitudel(x))\n",
    "    longitude.append(longitudel(y))\n",
    "\n",
    "Current=pd.DataFrame(zip(time,Locate,magni,depth,latitude,longitude), columns=['time','Location','Magnitude','Depth (km)','lattitude','longitude'])\n",
    "Current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also took into account the gdp of each country as compared to its population to get the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.worldometers.info/gdp/gdp-by-country/'\n",
    "response=get(url)\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "Head=soup.find('thead')\n",
    "H=Head.find_all('th')\n",
    "Table=soup.find('tbody')\n",
    "T=Table.find_all('tr')\n",
    "columns=[col for col in T[0]]\n",
    "Beep=[T[i] for i in range(0,len(T))]\n",
    "headers=[i.get_text() for i in H]\n",
    "\n",
    "tipping=[]\n",
    "for col in columns:\n",
    "    if col==' ':\n",
    "        pass\n",
    "    else:\n",
    "        tipping.append(col.get_text())\n",
    "tip=[]\n",
    "for i in range(0,len(T)):\n",
    "    clip=[]\n",
    "    columns=[col for col in T[i]]\n",
    "    for col in columns:\n",
    "        if col==' ':\n",
    "            pass\n",
    "        else:\n",
    "            clip.append(col.get_text())\n",
    "    tip.append(clip)\n",
    "gdp=pd.DataFrame(tip,columns=headers)\n",
    "\n",
    "#cleaning GDP nominal\n",
    "gdp['GDP (nominal, 2017) ']=gdp['GDP (nominal, 2017) '].str.replace('$',' ')\n",
    "gdp['GDP (nominal, 2017) ']=gdp['GDP (nominal, 2017) '].str.replace(',','').astype(float)\n",
    "gdp['GDP (nominal, 2017) ']=pd.to_numeric(gdp['GDP (nominal, 2017) '] )\n",
    "gdp.rename(columns={'GDP (nominal, 2017) ':'GDP($)'}, inplace=True)\n",
    "\n",
    "#Cleanng GDP abbrev\n",
    "gdp['GDP (abbrev.)']=gdp['GDP (abbrev.)'].str.replace('$',' ')\n",
    "def money(num,part):\n",
    "    if part=='trillion':\n",
    "        num=num*1000000000000\n",
    "    elif part=='billion':\n",
    "        num=num*1000000000\n",
    "    else:\n",
    "        num=num*1000000\n",
    "    return(num)\n",
    "\n",
    "def marker(plop):\n",
    "    mak=len(plop)//2\n",
    "    num=plop[0:mak]\n",
    "    word=plop[mak:len(plop)]\n",
    "    if word=='illion' or word=='rillion' :\n",
    "        num=float(plop[0:mak-1])\n",
    "        word=plop[mak-1:len(plop)]\n",
    "        return(money(num,word))\n",
    "    else:\n",
    "        return(money(float(num),word))\n",
    "gdp['GDP (abbrev.)']=[marker(i) for i in gdp['GDP (abbrev.)']]\n",
    "gdp.rename(columns={'GDP (abbrev.)':'GDP (abbrev.)($)'}, inplace=True)\n",
    "\n",
    "#clean growth\n",
    "gdp['GDP  growth']=gdp['GDP  growth'].str.replace('%','').astype(float).divide(100)\n",
    "gdp.rename(columns={'GDP  growth':'GDP  growth(%)'}, inplace=True)\n",
    "#clean population\n",
    "gdp['Population (2017) ']=gdp['Population (2017) '].str.replace(',','').astype(int)\n",
    "gdp.rename(columns={'Population (2017) ':'Population'}, inplace=True)\n",
    "#clean per capita\n",
    "gdp['GDP  per capita ']=gdp['GDP  per capita '].str.replace('$',' ').str.replace(',','').astype(int)\n",
    "gdp.rename(columns={'GDP  per capita ':'GDP  per capita ($)'}, inplace=True)\n",
    "#share of GDP\n",
    "gdp['Share of World GDP ']=gdp['Share of World GDP '].str.replace('%','').astype(float).divide(100)\n",
    "gdp.rename(columns={'Share of World GDP ':'Share of World GDP (%)'}, inplace=True)\n",
    "\n",
    "gdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add GDP, Population and GDP per captia to DF\n",
    "\n",
    "gdp_column = []\n",
    "pop_column = []\n",
    "gdp_per_cap_column = []\n",
    "\n",
    "for i in range(len(earthquake_data)):\n",
    "    country = earthquake_data.iloc[i, 1]\n",
    "    if country == \"Taiwan\":\n",
    "        gdp_column.append(586104000000)\n",
    "        pop_column.append(23780452)\n",
    "        gdp_per_cap_column.append(24828)\n",
    "    if country == \"Venezuela\":\n",
    "        gdp_column.append(70140000000)\n",
    "        pop_column.append(28887118)\n",
    "        gdp_per_cap_column.append(2548)\n",
    "    if country == \"Myamnar\":\n",
    "        gdp_column.append(355000000000)\n",
    "        pop_column.append(53582855)\n",
    "        gdp_per_cap_column.append(6707)\n",
    "    if country == \"Cayman Islands\":\n",
    "        gdp_column.append(4571000000)\n",
    "        pop_column.append(68076)\n",
    "        gdp_per_cap_column.append(70958)\n",
    "    for j in range(len(gdp)):\n",
    "        if gdp.iloc[j, 1] == country:\n",
    "            gdp_column.append(gdp.iloc[j, 2])\n",
    "            pop_column.append(gdp.iloc[j, 5])\n",
    "            gdp_per_cap_column.append(gdp.iloc[j, 6])\n",
    "            break\n",
    "\n",
    "#Insert columns onto dataframe\n",
    "earthquake_data.insert(loc = 13, column = 'GDP', value = gdp_column)\n",
    "earthquake_data.insert(loc = 14, column = 'Population', value = pop_column)\n",
    "earthquake_data.insert(loc = 15, column = 'GDP Per Capita', value = gdp_per_cap_column)\n",
    "\n",
    "earthquake_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are hoping as of right now that this will help us to determine some correlation between earthquakes and damage. The hope is to see a correlation between countries with higher gdps having less damage than those of lower gdp.\n",
    "<br>\n",
    "At some point we hope to be able to attach cities to some of the cordinates and see if we can get a good cluster of points to determine how often earthquakes strick at certain magnitudes and what damaged is caused from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Exploring the data through graphs before applying an analysis.\n",
    "$$\n",
    "$$\n",
    "First thing we wanted to do was to take our USGS data and apply it to a geomap to see if we could see a pattern of where earthquakes are more likley to happen. This is important especially when we look at regresion and clustering analysis. Since our 2017-2018 data has no longitude or lattitude points. We will procede with 2019-2020 data for cluster analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cool(m):\n",
    "    if 0<=m<1:\n",
    "        return('green')\n",
    "    elif 1<=m<2:\n",
    "        return('orange')\n",
    "    elif 2<=m<3:\n",
    "        return('blue')\n",
    "    elif 3<=m<4:\n",
    "        return('teal')\n",
    "    elif 5<=m<6:\n",
    "        return('purple')\n",
    "    elif 6<=m<7:\n",
    "        return('amber')\n",
    "    else:\n",
    "        return('red')\n",
    "    \n",
    "nom=ArcGIS()\n",
    "p=nom.geocode(\"Palais du Gouvernement,P.O. Box 4546,N'Djaména\")\n",
    "map=folium.Map(location=[p.latitude,p.longitude], zoom_start=2, tiles=\"Stamen Terrain\")\n",
    "\n",
    "html=\"\"\"<h4><b>Earquakes Info</b></h4>\n",
    "<p><b>Location: </b>%s</p>\n",
    "<p><b>Magnitude: </b>%s</p>\"\"\"\n",
    "fgv=folium.FeatureGroup(name=\"shake\")\n",
    "for la, lo, m,l in zip(lattitude,longitude,magni,Locate):\n",
    "    iframe=folium.IFrame(html=html %(l,str(m)),width=200,height=100)\n",
    "    fgv.add_child(folium.CircleMarker(location=[la,lo],fill_color=cool(m),popup=folium.Popup(iframe),fill=True,color='black',fill_capacity=0.9))\n",
    "map.add_child(fgv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=['Magnitude','Intensity (MMI / JMA)','Fatalities','Injuries','displaced','Impact value (D)','buildings damaged','buildings destroyed']\n",
    "scattering=earthquake_data[col]\n",
    "plt.figure(1, figsize = (15, 15))\n",
    "axs=pd.plotting.scatter_matrix(scattering)\n",
    "plt.suptitle('Earthquake Data', size=20)\n",
    "n = len(scattering.columns)\n",
    "for x in range(n):\n",
    "    for y in range(n):\n",
    "        # to get the axis of subplots\n",
    "        ax = axs[x, y]\n",
    "        # to make x axis name vertical  \n",
    "        ax.xaxis.label.set_rotation(90)\n",
    "        # to make y axis name horizontal \n",
    "        ax.yaxis.label.set_rotation(0)\n",
    "        # to make sure y axis names are outside the plot area\n",
    "        ax.yaxis.labelpad = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed several things about the data.  \n",
    "\n",
    "First of all, the data points with a magnitude of zero could skew any analysis, since they indicate impact without an earthquake occuring.  \n",
    "Second, many of the distributions seem to follow nonlinear and bell-like relationships. This makes us believe there is a factor(s) other than magnitude that affect the relationships. Additionally, the logrithmic behavior of magnitude might be affecting the linearity of the relationship. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we dive deeper into the earthquake data we find the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_data.groupby(['Country']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_data.groupby(['Intensity (MMI / JMA)']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we look more into the data we find that there are some patternsto be discovered. For example, we can see that over the course of 4 years, certain countries get hit harder than others with earthquakes. We will have to go into that deeper for this project and see which places we can focus our attention on.\n",
    "<br>\n",
    "When we look at intensity, we see a and expected a rise in magnitude with intensity as well as with everythng else. However, it can be noted that at intensity 9 we get a higher magnitude than intensity 10. We also see what intensities happen the most as well with 6 having the most compared to anyone else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis and Deviation\n",
    "**Final analysis:** We will havesome work to do on looking though our numbers and seeing the best corelations. This may include looking into the most hit places and seeing if we can find anything there. Our database data has been cleaned and is now easier to read, but with what we have scraped, we need to bring together and possiblly find a better corrolation there.\n",
    "$$\n",
    "$$\n",
    "**Deviation:** There was going to be a lot more data that we wanted to include. This came with the possibility of looking at 10 different cities and comparing population density, building grades, infrastructure, response/warning systems, and much more. However, this data has turned out to be a lot harder to find for any place. The amount of time it would take to find and scrape all of that data is not conducive with our time frame.\n",
    "$$\n",
    "$$\n",
    "**Peer Review Feedback:** We got feed back from Amber Kiser. During the peer review, we mostly discussed the most effective ways to determine and quanitify infrastruture for a given location. We discussed using infrastructure spending or some type of rating system. Additionally, she wondered about the types of regressions we would be running. We discussed the use of doing a strict linear, multi-variable regression in comparison with a nonlinear variation. We figured we would just have to play around with the data. Additionally, she agreed that exploring whether clustering affected the accuracy of the regressions was a good thing to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DONE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
