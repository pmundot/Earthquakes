{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Vibes: The Probability of Earthquake Magnitude and Death\n",
    "*A Milestone Report for the Introduction to Data Science Project*  \n",
    "  \n",
    "**Team Fugacity Members**  \n",
    "Paul Mundt, u0932146  \n",
    "Katie Jones, u0541901  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Cleaning\n",
    "\n",
    "The main source of data for the regressions comes from the Earthquake Impact Database on earthquake-report.com. For the analysis, we are analyzing data from 2017-2020. The data from 2020 is partial, and includes information that was availible on Wednesday, March 18th. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import scipy as sci\n",
    "from requests import get\n",
    "import folium\n",
    "from geopy.geocoders import ArcGIS\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as sm\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline  \n",
    "plt.rcParams['figure.figsize'] = (12, 12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'2017_raw.csv' does not exist: b'2017_raw.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-68f32d905ac4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import CSV Files Extracted from the Earthquake Impact Database\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata_2017\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2017_raw.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"ISO-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"all\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata_2018\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2018_raw.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"ISO-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"all\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata_2019\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2019_raw.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"ISO-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"all\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata_2020\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2020_raw.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"ISO-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"all\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'2017_raw.csv' does not exist: b'2017_raw.csv'"
     ]
    }
   ],
   "source": [
    "# Import CSV Files Extracted from the Earthquake Impact Database\n",
    "data_2017 = pd.read_csv(\"2017_raw.csv\", encoding = \"ISO-8859-1\").dropna(axis = 0, how = \"all\")\n",
    "data_2018 = pd.read_csv(\"2018_raw.csv\", encoding = \"ISO-8859-1\").dropna(axis = 0, how = \"all\")\n",
    "data_2019 = pd.read_csv(\"2019_raw.csv\", encoding = \"ISO-8859-1\").dropna(axis = 0, how = \"all\")\n",
    "data_2020 = pd.read_csv(\"2020_raw.csv\", encoding = \"ISO-8859-1\").dropna(axis = 0, how = \"all\", thresh = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean 2020 Data\n",
    "data_2020 = data_2020.drop('Depth (km)', axis = 1).drop('Type', axis = 1).drop('Origin', axis = 1).drop('Tsunami height', axis = 1)\n",
    "data_2020 = data_2020.fillna(value = 0)\n",
    "data_2020 = data_2020.rename(columns = {\"Epicenter\" : \"Country\", \"Region (Epicenter)\" : \"Region\"} )\n",
    "\n",
    "#Clean 2019 Data\n",
    "data_2019 = data_2019.drop('Depth (km)', axis = 1).drop('Type', axis = 1).drop('Origin', axis = 1).drop('Tsunami height', axis = 1)\n",
    "data_2019 = data_2019.fillna(value = 0).replace(\"--\", \"0\")\n",
    "data_2019 = data_2019.rename(columns = {\"Country (Epicenter)\" : \"Country\", \"Region (Epicenter)\" : \"Region\"} )\n",
    "\n",
    "#Clean 2018 Data\n",
    "data_2018 = data_2018.drop('Depth (km)', axis = 1).drop('Type', axis = 1).drop('Origin', axis = 1).drop('Tsunami height', axis = 1)\n",
    "data_2018 = data_2018.fillna(value = 0).replace(\"--\", \"0\")\n",
    "latlong_2018 = [0 for _ in range(0, len(data_2018.index))]\n",
    "data_2018.insert(loc = 4, column = 'Lat', value = latlong_2018)\n",
    "data_2018.insert(loc = 5, column = 'Long', value = latlong_2018)\n",
    "data_2018 = data_2018.rename(columns = {\"Country (Epicenter)\" : \"Country\", \"Region (Epicenter)\" : \"Region\"} )\n",
    "\n",
    "#Clean 2017 Data\n",
    "data_2017 = data_2017.drop('Depth (km)', axis = 1).drop('Tsunami height', axis = 1)\n",
    "data_2017 = data_2017.fillna(value = 0).replace(\"--\", \"0\")\n",
    "latlong_2017 = [0 for _ in range(0, len(data_2017.index))]\n",
    "data_2017.insert(loc = 4, column = 'Lat', value = latlong_2017)\n",
    "data_2017.insert(loc = 5, column = 'Long', value = latlong_2017)\n",
    "data_2017 = data_2017.rename(columns = {\"Impact coefficient (D)\" : \"Impact value (D)\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate Dataframes into a Single Dataset\n",
    "earthquake_data = pd.concat([data_2017, data_2018, data_2019, data_2020], sort = False).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_data['Intensity (MMI / JMA)']=earthquake_data['Intensity (MMI / JMA)'].str.replace('+',' ')\n",
    "earthquake_data['Intensity (MMI / JMA)']=earthquake_data['Intensity (MMI / JMA)'].str.replace('-',' ')\n",
    "earthquake_data['Intensity (MMI / JMA)']=earthquake_data['Intensity (MMI / JMA)'].str.replace('JMA',' ')\n",
    "earthquake_data['Intensity (MMI / JMA)']=earthquake_data['Intensity (MMI / JMA)'].str.replace('/ 7',' ')\n",
    "earthquake_data['Intensity (MMI / JMA)']=earthquake_data['Intensity (MMI / JMA)'].str.replace('Shindo ','')\n",
    "\n",
    "def roman_to_number(num):\n",
    "    rom_val = {'I': 1, 'V': 5, 'X': 10, 'L':50 , 'C':100, 'D':500, 'M':1000}\n",
    "    res=0\n",
    "    i=0\n",
    "    while i<len(num):\n",
    "        s1=num[i]\n",
    "        num1=rom_val[s1]\n",
    "        if i+1<len(num):\n",
    "            s2=num[i+1]\n",
    "            num2=rom_val[s2]\n",
    "            if num2<=num1:\n",
    "                res=res+num1\n",
    "                i+=1\n",
    "            else:\n",
    "                res=res+num2-num1\n",
    "                i+=2\n",
    "        else:\n",
    "            res=res+num1\n",
    "            i+=1\n",
    "    return(res)\n",
    "        \n",
    "    \n",
    "           \n",
    "def change(num):\n",
    "    if type(num)==str:\n",
    "        if len(num)>4:\n",
    "            tip=num[0:len(num)//2].strip()\n",
    "        else:\n",
    "            tip=num.strip()\n",
    "        try:\n",
    "            return(int(tip))\n",
    "        except:\n",
    "            return(roman_to_number(tip))\n",
    "    else:\n",
    "        return(num)\n",
    "earthquake_data['Intensity (MMI / JMA)']=[change(chest) for chest in earthquake_data['Intensity (MMI / JMA)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also able to get the current earthquake data from the USGS website and apply it to a webmap, with pop-ups. These pop-ups will hold future information for probability and other needed info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://feed.unmung.com/feed?feed=http://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/4.5+_month.atom'\n",
    "response=get(url)\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "Location=soup.find_all('a',class_=\"u-url\")\n",
    "Data=soup.find_all('span',class_=\"e-summary\")\n",
    "Location.pop(0)\n",
    "def mag_title(x):\n",
    "    magnitude=float(x[1:5])\n",
    "    location=x[7:]\n",
    "    return(magnitude,location)\n",
    "\n",
    "Locate=[]\n",
    "magni=[]\n",
    "for kip in Location:\n",
    "    inter=kip.get_text()\n",
    "    poper=mag_title(inter)\n",
    "    Locate.append(poper[1])\n",
    "    magni.append(poper[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time=[]\n",
    "loc=[]\n",
    "depth=[]\n",
    "for stuff in Data:\n",
    "    j=stuff.find_all('dd')\n",
    "    time.append(j[0].get_text())\n",
    "    loc.append(j[2].get_text())\n",
    "    depth.append(j[3].get_text())   \n",
    "\n",
    "def lattitudel(x):\n",
    "    if \"S\" in x:\n",
    "        cord=-float(x.replace('°S',''))\n",
    "    else:\n",
    "        cord=float(x.replace('°N',''))\n",
    "    return(cord)\n",
    "\n",
    "def longitudel(x):\n",
    "    if \"W\" in x:\n",
    "        cord=-float(x.replace('°W',''))\n",
    "    else:\n",
    "        cord=float(x.replace('°E',''))\n",
    "    return(cord)\n",
    "def coord_split(x):\n",
    "    half=len(x)//2\n",
    "    first=x[:half]\n",
    "    last=x[half:]\n",
    "    return(first,last)\n",
    "\n",
    "lattitude=[]\n",
    "longitude=[]\n",
    "for sets in loc:\n",
    "    x,y=coord_split(sets)\n",
    "    lattitude.append(lattitudel(x))\n",
    "    longitude.append(longitudel(y))\n",
    "\n",
    "Current=pd.DataFrame(zip(time,Locate,magni,depth,lattitude,longitude), columns=['time','Location','Magnitude','Depth (km)','lattitude','longitude'])\n",
    "Current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also took into account the gdp of each country as compared to its population to get the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.worldometers.info/gdp/gdp-by-country/'\n",
    "response=get(url)\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "Head=soup.find('thead')\n",
    "H=Head.find_all('th')\n",
    "Table=soup.find('tbody')\n",
    "T=Table.find_all('tr')\n",
    "columns=[col for col in T[0]]\n",
    "Beep=[T[i] for i in range(0,len(T))]\n",
    "headers=[i.get_text() for i in H]\n",
    "\n",
    "tipping=[]\n",
    "for col in columns:\n",
    "    if col==' ':\n",
    "        pass\n",
    "    else:\n",
    "        tipping.append(col.get_text())\n",
    "tip=[]\n",
    "for i in range(0,len(T)):\n",
    "    clip=[]\n",
    "    columns=[col for col in T[i]]\n",
    "    for col in columns:\n",
    "        if col==' ':\n",
    "            pass\n",
    "        else:\n",
    "            clip.append(col.get_text())\n",
    "    tip.append(clip)\n",
    "gdp=pd.DataFrame(tip,columns=headers)\n",
    "\n",
    "#cleaning GDP nominal\n",
    "gdp['GDP (nominal, 2017) ']=gdp['GDP (nominal, 2017) '].str.replace('$',' ')\n",
    "gdp['GDP (nominal, 2017) ']=gdp['GDP (nominal, 2017) '].str.replace(',','').astype(float)\n",
    "gdp['GDP (nominal, 2017) ']=pd.to_numeric(gdp['GDP (nominal, 2017) '] )\n",
    "gdp.rename(columns={'GDP (nominal, 2017) ':'GDP($)'}, inplace=True)\n",
    "\n",
    "#Cleanng GDP abbrev\n",
    "gdp['GDP (abbrev.)']=gdp['GDP (abbrev.)'].str.replace('$',' ')\n",
    "def money(num,part):\n",
    "    if part=='trillion':\n",
    "        num=num*1000000000000\n",
    "    elif part=='billion':\n",
    "        num=num*1000000000\n",
    "    else:\n",
    "        num=num*1000000\n",
    "    return(num)\n",
    "\n",
    "def marker(plop):\n",
    "    mak=len(plop)//2\n",
    "    num=plop[0:mak]\n",
    "    word=plop[mak:len(plop)]\n",
    "    if word=='illion' or word=='rillion' :\n",
    "        num=float(plop[0:mak-1])\n",
    "        word=plop[mak-1:len(plop)]\n",
    "        return(money(num,word))\n",
    "    else:\n",
    "        return(money(float(num),word))\n",
    "gdp['GDP (abbrev.)']=[marker(i) for i in gdp['GDP (abbrev.)']]\n",
    "gdp.rename(columns={'GDP (abbrev.)':'GDP (abbrev.)($)'}, inplace=True)\n",
    "\n",
    "#clean growth\n",
    "gdp['GDP  growth']=gdp['GDP  growth'].str.replace('%','').astype(float).divide(100)\n",
    "gdp.rename(columns={'GDP  growth':'GDP  growth(%)'}, inplace=True)\n",
    "#clean population\n",
    "gdp['Population (2017) ']=gdp['Population (2017) '].str.replace(',','').astype(int)\n",
    "gdp.rename(columns={'Population (2017) ':'Population'}, inplace=True)\n",
    "#clean per capita\n",
    "gdp['GDP  per capita ']=gdp['GDP  per capita '].str.replace('$',' ').str.replace(',','').astype(int)\n",
    "gdp.rename(columns={'GDP  per capita ':'GDP  per capita ($)'}, inplace=True)\n",
    "#share of GDP\n",
    "gdp['Share of World GDP ']=gdp['Share of World GDP '].str.replace('%','').astype(float).divide(100)\n",
    "gdp.rename(columns={'Share of World GDP ':'Share of World GDP (%)'}, inplace=True)\n",
    "\n",
    "gdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add GDP, Population and GDP per captia to DF\n",
    "\n",
    "gdp_column = []\n",
    "pop_column = []\n",
    "gdp_per_cap_column = []\n",
    "\n",
    "print(len(earthquake_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are hoping as of right now that this will help us to determine some correlation between earthquakes and damage. The hope is to see a correlation between countries with higher gdps having less damage than those of lower gdp.\n",
    "<br>\n",
    "At some point we hope to be able to attach cities to some of the cordinates and see if we can get a good cluster of points to determine how often earthquakes strick at certain magnitudes and what damaged is caused from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Exploring the data through graphs before applying an analysis.\n",
    "$$\n",
    "$$\n",
    "First thing we wanted to do was to take our USGS data and apply it to a geomap to see if we could see a pattern of where earthquakes are more likley to happen. This is important especially when we look at regresion and clustering analysis. Since our 2017-2018 data has no longitude or lattitude points. We will procede with 2019-2020 data for cluster analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cool(m):\n",
    "    if 0<=m<1:\n",
    "        return('green')\n",
    "    elif 1<=m<2:\n",
    "        return('orange')\n",
    "    elif 2<=m<3:\n",
    "        return('blue')\n",
    "    elif 3<=m<4:\n",
    "        return('teal')\n",
    "    elif 5<=m<6:\n",
    "        return('purple')\n",
    "    elif 6<=m<7:\n",
    "        return('amber')\n",
    "    else:\n",
    "        return('red')\n",
    "    \n",
    "nom=ArcGIS()\n",
    "p=nom.geocode(\"Palais du Gouvernement,P.O. Box 4546,N'Djaména\")\n",
    "map=folium.Map(location=[p.latitude,p.longitude], zoom_start=2, tiles=\"Stamen Terrain\")\n",
    "\n",
    "html=\"\"\"<h4><b>Earquakes Info</b></h4>\n",
    "<p><b>Location: </b>%s</p>\n",
    "<p><b>Magnitude: </b>%s</p>\"\"\"\n",
    "fgv=folium.FeatureGroup(name=\"shake\")\n",
    "for la, lo, m,l in zip(lattitude,longitude,magni,Locate):\n",
    "    iframe=folium.IFrame(html=html %(l,str(m)),width=200,height=100)\n",
    "    fgv.add_child(folium.CircleMarker(location=[la,lo],fill_color=cool(m),popup=folium.Popup(iframe),fill=True,color='black',fill_capacity=0.9))\n",
    "map.add_child(fgv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=['Magnitude','Intensity (MMI / JMA)','Fatalities','Injuries','displaced','Impact value (D)','buildings damaged','buildings destroyed']\n",
    "scattering=earthquake_data[col]\n",
    "axs=pd.plotting.scatter_matrix(scattering)\n",
    "plt.suptitle('Earthquake Data', size=20)\n",
    "n = len(scattering.columns)\n",
    "for x in range(n):\n",
    "    for y in range(n):\n",
    "        # to get the axis of subplots\n",
    "        ax = axs[x, y]\n",
    "        # to make x axis name vertical  \n",
    "        ax.xaxis.label.set_rotation(90)\n",
    "        # to make y axis name horizontal \n",
    "        ax.yaxis.label.set_rotation(0)\n",
    "        # to make sure y axis names are outside the plot area\n",
    "        ax.yaxis.labelpad = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed several things about the data.  \n",
    "\n",
    "First of all, the data points with a magnitude of zero could skew any analysis, since they indicate impact without an earthquake occuring.  \n",
    "Second, many of the distributions seem to follow nonlinear and bell-like relationships. This makes us believe there is a factor(s) other than magnitude that affect the relationships. Additionally, the logrithmic behavior of magnitude might be affecting the linearity of the relationship. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we dive deeper into the earthquake data we find the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_data.groupby(['Country']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_data.groupby(['Intensity (MMI / JMA)']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we look more into the data we find that there are some patternsto be discovered. For example, we can see that over the course of 4 years, certain countries get hit harder than others with earthquakes. We will have to go into that deeper for this project and see which places we can focus our attention on.\n",
    "<br>\n",
    "When we look at intensity, we see a and expected a rise in magnitude with intensity as well as with everythng else. However, it can be noted that at intensity 9 we get a higher magnitude than intensity 10. We also see what intensities happen the most as well with 6 having the most compared to anyone else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis and Deviation\n",
    "**Final analysis:** We will havesome work to do on looking though our numbers and seeing the best corelations. This may include looking into the most hit places and seeing if we can find anything there. Our database data has been cleaned and is now easier to read, but with what we have scraped, we need to bring together and possiblly find a better corrolation there.\n",
    "$$\n",
    "$$\n",
    "**Deviation:** There was going to be a lot more data that we wanted to include. This came with the possibility of looking at 10 different cities and comparing population density, building grades, infrastructure, response/warning systems, and much more. However, this data has turned out to be a lot harder to find for any place. The amount of time it would take to find and scrape all of that data is not conducive with our time frame.\n",
    "$$\n",
    "$$\n",
    "**Peer Review Feedback:** We got feed back from Amber Kiser. During the peer review, we mostly discussed the most effective ways to determine and quanitify infrastruture for a given location. We discussed using infrastructure spending or some type of rating system. Additionally, she wondered about the types of regressions we would be running. We discussed the use of doing a strict linear, multi-variable regression in comparison with a nonlinear variation. We figured we would just have to play around with the data. Additionally, she agreed that exploring whether clustering affected the accuracy of the regressions was a good thing to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
