{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Vibes: The Probability of Earthquake Magnitude and Death\n",
    "*The Final Report for the Introduction to Data Science Project*  \n",
    "  \n",
    "**Team Fugacity Members**  \n",
    "Paul Mundt, u0932146  \n",
    "Katie Jones, u0541901  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction [Check Off: Katie, Paul]\n",
    "\n",
    "### Motivations [Edits Needed]\n",
    "  \n",
    "Last year on February 15th, 2019 at 05:09am (MST) a 3.7M earthquake hit Bluffdale, UT. The magnitude was enough for residents in neighboring cities such as Draper and Riverton, to feel the ground shake. This caused many people in the area to wonder about their safety as well as their preparedness for if another earthquake were to occur. As residents of a city built on a faultline, we are keenly aware of the ubiquity of earthquakes as a disruption life in Salt Lake City.  \n",
    "  \n",
    "Earthquakes happen all over the world, most of which we cannot feel. As citizens living along a major faultline, the question of earthquake magnitude and danger is always a concern. This project will look at the historical data of earthquakes around the globe and determine the danger of displacement, property damage, injury, and death (called impact factors from this point forward) in relation to earthquake magnitude and other socioeconomic factors.\n",
    "  \n",
    "  \n",
    "### Project Objectives [Edits Needed]\n",
    "  \n",
    "For this project, we are trying to answer the following questions about earthquakes and their impacts:\n",
    "\n",
    "**1**) Can impact factors be predicted based on magnitude of the earthquake?\n",
    "  \n",
    "**2**) What is the probability of displacement, property damage, injury, or death in an earthquake?\n",
    "  \n",
    "**3**) How do socioeconomic factors, like GDP of the country, population density, etc, affects the accuracy of impact factor predictions?\n",
    "  \n",
    "**4**) Does clustering of data prior to analysis increase the accuracy of the predictive models? \n",
    "  \n",
    "We will be investigating these claims by analyzing seismographic information about earthquakes, information about effects of earthquakes, and looking at a variety of other data types (national GDP, population, population density, etc) . \n",
    " \n",
    "Additionally, we hope to apply the data collected to a geomap for a visualization of earthquake locations and severity. \n",
    "  \n",
    "### Ethical Considerations [Edits Needed]\n",
    "  \n",
    "The main stakeholders in a data science project like this are general civilians and people involved in infrastructure (architects, city planners, civil engineers, law makers, etc). \n",
    "To the general citizen, this project can be both incredibly informative as well as fear inducing. Most people only gain exposure to earthquakes after a major disaster. Because of this, many people inherently fear earthquakes and the impact that they have on daily life. For this project, it is important that all correlations and statistics we explore are accurately and accessible explained and given context. As is often the case, the context given to the data is usually more important than what the data itself says. \n",
    "  \n",
    "This project will also be useful for people involved in infrastructure. By showing what variables that affect impact factors, the people incharge of building structures in a city might be better informed on the need for earthquake resistant structures. Areas of frequent high magnitude earthquakes are supposedly safer than those with the semi-active seismic activity. We believe this is because infrastructure and response are more highly valued when building in the area, as more frequent danger incentivises buildings to be designed to withstand move intense oscillation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Cleaning [Check Off: Katie, Paul]\n",
    "\n",
    "### Earthquakes and Impact Factor Data [Edits Needed]\n",
    "The primary source of data was the Earthquake Impact Database from earthquake-report.com. The database from the website is linked to a set of Goolge sheets that can be saved as a CSV. The CSV files had to be copied and saved to replace formulas with text. This was done using Excel's specialty paste functionality.  \n",
    "The data was collected from January 1,2017 to March 17th, 2020. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all packages and functions needed for entire analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import scipy as sci\n",
    "from requests import get\n",
    "import folium\n",
    "from geopy.geocoders import ArcGIS\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as sm\n",
    "import pycountry\n",
    "import re\n",
    "from scipy.cluster import hierarchy \n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA \n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline  \n",
    "plt.rcParams['figure.figsize'] = (12, 12) \n",
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CSV Files Extracted from the Earthquake Impact Database\n",
    "data_2017 = pd.read_csv(\"2017_raw.csv\", encoding = \"ISO-8859-1\").dropna(axis = 0, how = \"all\")\n",
    "data_2018 = pd.read_csv(\"2018_raw.csv\", encoding = \"ISO-8859-1\").dropna(axis = 0, how = \"all\")\n",
    "data_2019 = pd.read_csv(\"2019_raw.csv\", encoding = \"ISO-8859-1\").dropna(axis = 0, how = \"all\")\n",
    "data_2020 = pd.read_csv(\"2020_raw.csv\", encoding = \"ISO-8859-1\").dropna(axis = 0, how = \"all\", thresh = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean 2017 Data\n",
    "data_2017 = data_2017.drop('Depth (km)', axis = 1).drop('Tsunami height', axis = 1)\n",
    "data_2017 = data_2017.fillna(value = 0).replace(\"--\", \"0\")\n",
    "latlong_2017 = [0 for _ in range(0, len(data_2017.index))]\n",
    "data_2017.insert(loc = 4, column = 'Lat', value = latlong_2017)\n",
    "data_2017.insert(loc = 5, column = 'Long', value = latlong_2017)\n",
    "data_2017 = data_2017.rename(columns = {\"Impact coefficient (D)\" : \"Impact value (D)\"} )\n",
    "\n",
    "# Clean 2018 Data\n",
    "data_2018 = data_2018.drop('Depth (km)', axis = 1).drop('Type', axis = 1).drop('Origin', axis = 1).drop('Tsunami height', axis = 1)\n",
    "data_2018 = data_2018.fillna(value = 0).replace(\"--\", \"0\")\n",
    "latlong_2018 = [0 for _ in range(0, len(data_2018.index))]\n",
    "data_2018.insert(loc = 4, column = 'Lat', value = latlong_2018)\n",
    "data_2018.insert(loc = 5, column = 'Long', value = latlong_2018)\n",
    "data_2018 = data_2018.rename(columns = {\"Country (Epicenter)\" : \"Country\", \"Region (Epicenter)\" : \"Region\"} )\n",
    "\n",
    "# Clean 2019 Data\n",
    "data_2019 = data_2019.drop('Depth (km)', axis = 1).drop('Type', axis = 1).drop('Origin', axis = 1).drop('Tsunami height', axis = 1)\n",
    "data_2019 = data_2019.fillna(value = 0).replace(\"--\", \"0\")\n",
    "data_2019 = data_2019.rename(columns = {\"Country (Epicenter)\" : \"Country\", \"Region (Epicenter)\" : \"Region\"} )\n",
    "\n",
    "# Clean 2020 Data\n",
    "data_2020 = data_2020.drop('Depth (km)', axis = 1).drop('Type', axis = 1).drop('Origin', axis = 1).drop('Tsunami height', axis = 1)\n",
    "data_2020 = data_2020.fillna(value = 0)\n",
    "data_2020 = data_2020.rename(columns = {\"Epicenter\" : \"Country\", \"Region (Epicenter)\" : \"Region\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate Dataframes into a Single Dataset\n",
    "earthquake_data = pd.concat([data_2017, data_2018, data_2019, data_2020], sort = False).reset_index(drop = True)\n",
    "Database = pd.concat([data_2019, data_2020], sort = False).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"Intensity (MMI / JMA)\" Column from Roman Numerals to Integers\n",
    "earthquake_data['Intensity (MMI / JMA)']=earthquake_data['Intensity (MMI / JMA)'].str.replace('+',' ')\n",
    "earthquake_data['Intensity (MMI / JMA)']=earthquake_data['Intensity (MMI / JMA)'].str.replace('-',' ')\n",
    "earthquake_data['Intensity (MMI / JMA)']=earthquake_data['Intensity (MMI / JMA)'].str.replace('JMA',' ')\n",
    "earthquake_data['Intensity (MMI / JMA)']=earthquake_data['Intensity (MMI / JMA)'].str.replace('/ 7',' ')\n",
    "earthquake_data['Intensity (MMI / JMA)']=earthquake_data['Intensity (MMI / JMA)'].str.replace('Shindo ','')\n",
    "\n",
    "Database['Intensity (MMI / JMA)']=Database['Intensity (MMI / JMA)'].str.replace('+',' ')\n",
    "Database['Intensity (MMI / JMA)']=Database['Intensity (MMI / JMA)'].str.replace('-',' ')\n",
    "Database['Intensity (MMI / JMA)']=Database['Intensity (MMI / JMA)'].str.replace('JMA',' ')\n",
    "Database['Intensity (MMI / JMA)']=Database['Intensity (MMI / JMA)'].str.replace('/ 7',' ')\n",
    "Database['Intensity (MMI / JMA)']=Database['Intensity (MMI / JMA)'].str.replace('Shindo ','')\n",
    "\n",
    "def roman_to_number(num):\n",
    "    rom_val = {'I': 1, 'V': 5, 'X': 10, 'L':50 , 'C':100, 'D':500, 'M':1000}\n",
    "    res=0\n",
    "    i=0\n",
    "    while i<len(num):\n",
    "        s1=num[i]\n",
    "        num1=rom_val[s1]\n",
    "        if i+1<len(num):\n",
    "            s2=num[i+1]\n",
    "            num2=rom_val[s2]\n",
    "            if num2<=num1:\n",
    "                res=res+num1\n",
    "                i+=1\n",
    "            else:\n",
    "                res=res+num2-num1\n",
    "                i+=2\n",
    "        else:\n",
    "            res=res+num1\n",
    "            i+=1\n",
    "    return(res)\n",
    "        \n",
    "    \n",
    "           \n",
    "def change(num):\n",
    "    if type(num)==str:\n",
    "        if len(num)>4:\n",
    "            tip=num[0:len(num)//2].strip()\n",
    "        else:\n",
    "            tip=num.strip()\n",
    "        try:\n",
    "            return(int(tip))\n",
    "        except:\n",
    "            return(roman_to_number(tip))\n",
    "    else:\n",
    "        return(num)\n",
    "earthquake_data['Intensity (MMI / JMA)']=[change(chest) for chest in earthquake_data['Intensity (MMI / JMA)']]\n",
    "Database['Intensity (MMI / JMA)']=[change(chest) for chest in Database['Intensity (MMI / JMA)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Cleaned DataFrame\n",
    "earthquake_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Socioeconomic Information\n",
    "Socioeconomic information based on country was scraped from World-o-meters.info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Socioeconomic Information\n",
    "url = 'https://www.worldometers.info/gdp/gdp-by-country/'\n",
    "response=get(url)\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "Head=soup.find('thead')\n",
    "H=Head.find_all('th')\n",
    "Table=soup.find('tbody')\n",
    "T=Table.find_all('tr')\n",
    "columns=[col for col in T[0]]\n",
    "Beep=[T[i] for i in range(0,len(T))]\n",
    "headers=[i.get_text() for i in H]\n",
    "\n",
    "tipping=[]\n",
    "for col in columns:\n",
    "    if col==' ':\n",
    "        pass\n",
    "    else:\n",
    "        tipping.append(col.get_text())\n",
    "tip=[]\n",
    "for i in range(0,len(T)):\n",
    "    clip=[]\n",
    "    columns=[col for col in T[i]]\n",
    "    for col in columns:\n",
    "        if col==' ':\n",
    "            pass\n",
    "        else:\n",
    "            clip.append(col.get_text())\n",
    "    tip.append(clip)\n",
    "gdp=pd.DataFrame(tip,columns=headers)\n",
    "\n",
    "#cleaning GDP nominal\n",
    "gdp['GDP (nominal, 2017) ']=gdp['GDP (nominal, 2017) '].str.replace('$',' ')\n",
    "gdp['GDP (nominal, 2017) ']=gdp['GDP (nominal, 2017) '].str.replace(',','').astype(float)\n",
    "gdp['GDP (nominal, 2017) ']=pd.to_numeric(gdp['GDP (nominal, 2017) '] )\n",
    "gdp.rename(columns={'GDP (nominal, 2017) ':'GDP($)'}, inplace=True)\n",
    "\n",
    "#Cleanng GDP abbrev\n",
    "gdp['GDP (abbrev.)']=gdp['GDP (abbrev.)'].str.replace('$',' ')\n",
    "def money(num,part):\n",
    "    if part=='trillion':\n",
    "        num=num*1000000000000\n",
    "    elif part=='billion':\n",
    "        num=num*1000000000\n",
    "    else:\n",
    "        num=num*1000000\n",
    "    return(num)\n",
    "\n",
    "def marker(plop):\n",
    "    mak=len(plop)//2\n",
    "    num=plop[0:mak]\n",
    "    word=plop[mak:len(plop)]\n",
    "    if word=='illion' or word=='rillion' :\n",
    "        num=float(plop[0:mak-1])\n",
    "        word=plop[mak-1:len(plop)]\n",
    "        return(money(num,word))\n",
    "    else:\n",
    "        return(money(float(num),word))\n",
    "gdp['GDP (abbrev.)']=[marker(i) for i in gdp['GDP (abbrev.)']]\n",
    "gdp.rename(columns={'GDP (abbrev.)':'GDP (abbrev.)($)'}, inplace=True)\n",
    "\n",
    "#clean growth\n",
    "gdp['GDP  growth']=gdp['GDP  growth'].str.replace('%','').astype(float).divide(100)\n",
    "gdp.rename(columns={'GDP  growth':'GDP  growth(%)'}, inplace=True)\n",
    "#clean population\n",
    "gdp['Population (2017) ']=gdp['Population (2017) '].str.replace(',','').astype(int)\n",
    "gdp.rename(columns={'Population (2017) ':'Population'}, inplace=True)\n",
    "#clean per capita\n",
    "gdp['GDP  per capita ']=gdp['GDP  per capita '].str.replace('$',' ').str.replace(',','').astype(int)\n",
    "gdp.rename(columns={'GDP  per capita ':'GDP  per capita ($)'}, inplace=True)\n",
    "#share of GDP\n",
    "gdp['Share of World GDP ']=gdp['Share of World GDP '].str.replace('%','').astype(float).divide(100)\n",
    "gdp.rename(columns={'Share of World GDP ':'Share of World GDP (%)'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print DataFrame\n",
    "gdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Socioeconomic Data to Earthquake Data\n",
    "Next we used the \"GDP\" DataFrame as a database of sorts. The earthquake data has the country of epicenter listed, so that was compared to the list from World-o-meter.info and the GDP, GDP per capita, and Population were added to the earthquake data.  \n",
    "In the World-o-meter.info data, information about 4 countries were missing: Taiwan, Venezuela, Myamnar, and Cayman Islands. The pertinent information about these countries was collected from their Wikipedia pages and added manually to earthquakes located in those countires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add GDP, Population and GDP per captia to lists for addition to DF\n",
    "\n",
    "gdp_column = []\n",
    "pop_column = []\n",
    "gdp_per_cap_column = []\n",
    "\n",
    "for i in range(len(earthquake_data)):\n",
    "    country = earthquake_data.iloc[i, 1]\n",
    "    if country == \"Taiwan\":\n",
    "        gdp_column.append(586104000000)\n",
    "        pop_column.append(23780452)\n",
    "        gdp_per_cap_column.append(24828)\n",
    "    if country == \"Venezuela\":\n",
    "        gdp_column.append(70140000000)\n",
    "        pop_column.append(28887118)\n",
    "        gdp_per_cap_column.append(2548)\n",
    "    if country == \"Myamnar\":\n",
    "        gdp_column.append(355000000000)\n",
    "        pop_column.append(53582855)\n",
    "        gdp_per_cap_column.append(6707)\n",
    "    if country == \"Cayman Islands\":\n",
    "        gdp_column.append(4571000000)\n",
    "        pop_column.append(68076)\n",
    "        gdp_per_cap_column.append(70958)\n",
    "    for j in range(len(gdp)):\n",
    "        if gdp.iloc[j, 1] == country:\n",
    "            gdp_column.append(gdp.iloc[j, 2])\n",
    "            pop_column.append(gdp.iloc[j, 5])\n",
    "            gdp_per_cap_column.append(gdp.iloc[j, 6])\n",
    "            break\n",
    "\n",
    "# Insert columns onto dataframe\n",
    "earthquake_data.insert(loc = 13, column = 'GDP', value = gdp_column)\n",
    "earthquake_data.insert(loc = 14, column = 'Population', value = pop_column)\n",
    "earthquake_data.insert(loc = 15, column = 'GDPPerCapita', value = gdp_per_cap_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Countries related to continents and land area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'https://www.worldatlas.com/cntycont.htm'\n",
    "url2='https://www.nationmaster.com/country-info/stats/Geography/Land-area/Square-miles'\n",
    "\n",
    "response=get(url1)\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "Tropicanna=soup.find('div',class_=\"miscTxt\")\n",
    "continents=Tropicanna.find_all('h2')\n",
    "cont_counts=Tropicanna.find_all('ul')\n",
    "top=cont_counts[1].find_all('li')\n",
    "Conts=[con.get_text() for con in continents]\n",
    "Countries=[]\n",
    "Continets=[]\n",
    "for j in range(len(Conts)-2):\n",
    "    stuff=Conts[j]\n",
    "    top=cont_counts[j].find_all('li')\n",
    "    for steel in top:\n",
    "        Countries.append(steel.get_text())\n",
    "        Continets.append(stuff)\n",
    "\n",
    "ConCun=pd.DataFrame(zip(Countries,Continets), columns=['Country','Continents'])\n",
    "ConCun['Continents']=ConCun['Continents'].str.replace('(','').str.replace(')','').str.replace('\\n','')\n",
    "ConCun['Continents']=[title[0:len(title)-3] for title in ConCun['Continents']]\n",
    "\n",
    "response1=get(url2)\n",
    "sauce=BeautifulSoup(response1.text,'html.parser')\n",
    "\n",
    "tim=sauce.find('tbody')\n",
    "info=tim.find_all('tr')\n",
    "country=[]\n",
    "land=[]\n",
    "for i in range(0,len(info)):\n",
    "    test=info[i].find_all('td')\n",
    "    country.append(test[1].get_text())\n",
    "    land.append(test[2].get_text().strip())\n",
    "    \n",
    "landmass=pd.DataFrame(zip(country,land), columns=['Country','Landarea'])\n",
    "landmass['Country']=landmass['Country'].str.replace('\\n','')\n",
    "Information=ConCun.join(landmass.set_index('Country'), on='Country')\n",
    "\n",
    "Information['Landarea']=Information['Landarea'].str.replace('square miles','').str.replace(',','')\n",
    "time=[]\n",
    "#for x in Information['Landarea']:\n",
    "#    if 'million' in x:\n",
    "#        time.append(float(x[0:4])*1000000)\n",
    "#    else:\n",
    "#        time.append(float(x))\n",
    "\n",
    "        \n",
    "#Information['Landarea']=time\n",
    "earthquake_data=earthquake_data.join(Information.set_index('Country'), on='Country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print final DataFrame\n",
    "earthquake_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration [Check Off: Katie, Paul]\n",
    "  \n",
    "The first method of data exploration was to look at a geo map of earthquake data collected from the United States Geological Society (USGS) website. This geomap was used to see if we could visually see any patterns that might inform the clustering a regression analysis we did later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because of the frequency of earthquakes, USGS data takes all earthquake data all over the world and gives us the magnitude, longitude\n",
    "#latitude, and depth. From there we can plot these on a Geomap or scatter plot and see if any patterns come up\n",
    "url = 'http://feed.unmung.com/feed?feed=http://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/4.5+_month.atom'\n",
    "response=get(url)\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "Location=soup.find_all('a',class_=\"u-url\")\n",
    "Data=soup.find_all('span',class_=\"e-summary\")\n",
    "Location.pop(0)\n",
    "def mag_title(x):\n",
    "    magnitude=float(x[1:5])\n",
    "    location=x[7:]\n",
    "    return(magnitude,location)\n",
    "\n",
    "Locate=[]\n",
    "magni=[]\n",
    "for kip in Location:\n",
    "    inter=kip.get_text()\n",
    "    poper=mag_title(inter)\n",
    "    Locate.append(poper[1])\n",
    "    magni.append(poper[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continueing to clean the data\n",
    "\n",
    "time=[]\n",
    "loc=[]\n",
    "depth=[]\n",
    "for stuff in Data:\n",
    "    j=stuff.find_all('dd')\n",
    "    time.append(j[0].get_text())\n",
    "    loc.append(j[2].get_text())\n",
    "    depth.append(j[3].get_text())   \n",
    "\n",
    "def latitudel(x):\n",
    "    if \"S\" in x:\n",
    "        cord=-float(x.replace('°S',''))\n",
    "    else:\n",
    "        cord=float(x.replace('°N',''))\n",
    "    return(cord)\n",
    "\n",
    "def longitudel(x):\n",
    "    if \"W\" in x:\n",
    "        cord=-float(x.replace('°W',''))\n",
    "    else:\n",
    "        cord=float(x.replace('°E',''))\n",
    "    return(cord)\n",
    "def coord_split(x):\n",
    "    half=len(x)//2\n",
    "    first=x[:half]\n",
    "    last=x[half:]\n",
    "    return(first,last)\n",
    "\n",
    "latitude=[]\n",
    "longitude=[]\n",
    "for sets in loc:\n",
    "    x,y=coord_split(sets)\n",
    "    latitude.append(latitudel(x))\n",
    "    longitude.append(longitudel(y))\n",
    "\n",
    "Current=pd.DataFrame(zip(time,Locate,magni,depth,latitude,longitude), columns=['time','Location','Magnitude','Depth (km)','latitude','longitude'])\n",
    "\n",
    "def co(text):\n",
    "    for country in pycountry.countries:\n",
    "        if country.name in text:\n",
    "            return(country.name)\n",
    "t=[]\n",
    "for i,text in enumerate(Current['Location']):\n",
    "    j=re.findall(r\",([\\w\\s]+)\",text)\n",
    "    if len(j)==0:\n",
    "        if type(co(text)) is None:\n",
    "            t.append(co(text))\n",
    "        else:\n",
    "            t.append(text)\n",
    "    elif len(j)>0:\n",
    "        t.extend(j)\n",
    "    else:\n",
    "        t.append(text)\n",
    "Current['Country']=t\n",
    "Current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display geomap of USGS Data\n",
    "def cool(m):\n",
    "    if 0<=m<1:\n",
    "        return('green')\n",
    "    elif 1<=m<2:\n",
    "        return('orange')\n",
    "    elif 2<=m<3:\n",
    "        return('blue')\n",
    "    elif 3<=m<4:\n",
    "        return('teal')\n",
    "    elif 5<=m<6:\n",
    "        return('purple')\n",
    "    elif 6<=m<7:\n",
    "        return('amber')\n",
    "    else:\n",
    "        return('red')\n",
    "    \n",
    "nom=ArcGIS()\n",
    "p=nom.geocode(\"Palais du Gouvernement,P.O. Box 4546,N'Djaména\")\n",
    "map=folium.Map(location=[p.latitude,p.longitude], zoom_start=2, tiles=\"Stamen Terrain\")\n",
    "\n",
    "html=\"\"\"<h4><b>Earquakes Info</b></h4>\n",
    "<p><b>Location: </b>%s</p>\n",
    "<p><b>Magnitude: </b>%s</p>\"\"\"\n",
    "fgv=folium.FeatureGroup(name=\"shake\")\n",
    "for la, lo, m,l in zip(latitude,longitude,magni,Locate):\n",
    "    iframe=folium.IFrame(html=html %(l,str(m)),width=200,height=100)\n",
    "    fgv.add_child(folium.CircleMarker(location=[la,lo],fill_color=cool(m),popup=folium.Popup(iframe),fill=True,color='black',fill_capacity=0.9))\n",
    "map.add_child(fgv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Matrix\n",
    "Next method of data exploration is to create a cattering matrix. This allowed us to visually see coorelations between different variables of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=['Magnitude','Intensity (MMI / JMA)','Fatalities','Injuries','displaced','Impact value (D)','buildings damaged','buildings destroyed']\n",
    "scattering=earthquake_data[col]\n",
    "plt.figure(1, figsize = (15, 15))\n",
    "axs=pd.plotting.scatter_matrix(scattering)\n",
    "plt.suptitle('Earthquake Data', size=20)\n",
    "n = len(scattering.columns)\n",
    "for x in range(n):\n",
    "    for y in range(n):\n",
    "        # to get the axis of subplots\n",
    "        ax = axs[x, y]\n",
    "        # to make x axis name vertical  \n",
    "        ax.xaxis.label.set_rotation(90)\n",
    "        # to make y axis name horizontal \n",
    "        ax.yaxis.label.set_rotation(0)\n",
    "        # to make sure y axis names are outside the plot area\n",
    "        ax.yaxis.labelpad = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis [Check Off: Katie, Paul]\n",
    "  \n",
    "### Stage 1: Complete Data Regression\n",
    "Before beginning to cluster the data and group, we are running regressions on the entire data set as a whole. This is done to build a baseline of what can be predicted.  \n",
    "  \n",
    "First, we will complete a linear regression to find models for all impact factors as a function of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_data = earthquake_data.rename(columns = {\"Intensity (MMI / JMA)\" : \"Intensity\"} )\n",
    "earthquake_data = earthquake_data.rename(columns = {\"Impact value (D)\" : \"Impact\"} )\n",
    "earthquake_data = earthquake_data.rename(columns = {\"buildings damaged\" : \"Damaged\"} )\n",
    "earthquake_data = earthquake_data.rename(columns = {\"buildings destroyed\" : \"Destroyed\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_factors = []\n",
    "r_squared1 = []\n",
    "\n",
    "intensity_regress1 = sm.ols(formula = 'Intensity ~ Magnitude', data = earthquake_data).fit()\n",
    "impact_factors.append('Intensity')\n",
    "r_squared1.append(intensity_regress1.rsquared)\n",
    "\n",
    "fatalities_regress1 = sm.ols(formula = 'Fatalities ~ Magnitude', data = earthquake_data).fit()\n",
    "impact_factors.append('Fatalities')\n",
    "r_squared1.append(fatalities_regress1.rsquared)\n",
    "\n",
    "injuries_regress1 = sm.ols(formula = 'Injuries ~ Magnitude', data = earthquake_data).fit()\n",
    "impact_factors.append('Injuries')\n",
    "r_squared1.append(injuries_regress1.rsquared)\n",
    "\n",
    "displaced_regress1 = sm.ols(formula = 'displaced ~ Magnitude', data = earthquake_data).fit()\n",
    "impact_factors.append('Displaced')\n",
    "r_squared1.append(displaced_regress1.rsquared)\n",
    "\n",
    "impact_value_regress1 = sm.ols(formula = 'Impact ~ Magnitude', data = earthquake_data).fit()\n",
    "impact_factors.append('Impact Values')\n",
    "r_squared1.append(impact_value_regress1.rsquared)\n",
    "\n",
    "damaged_buildings_regress1 = sm.ols(formula = 'Damaged ~ Magnitude', data = earthquake_data).fit()\n",
    "impact_factors.append('Damaged Buildings')\n",
    "r_squared1.append(damaged_buildings_regress1.rsquared)\n",
    "\n",
    "destroyed_buildings_regress1 = sm.ols(formula = 'Destroyed ~ Magnitude', data = earthquake_data).fit()\n",
    "impact_factors.append('Destroyed Buildings')\n",
    "r_squared1.append(destroyed_buildings_regress1.rsquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R-Squared Value for Regressions Done on Magnitude Alone:\\n\")\n",
    "for i in range(len(impact_factors)):\n",
    "    print(\"The R-Squared Value for \" + impact_factors[i] + \" is \" + str(round(r_squared1[i], 3)) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, magnitude alone is not a good method to predict any of the impact factors we are interested in. This is a result we realized was likely. We believe there will be some level of dependence on a socioeconomic factor. This could be because things like GDP or GDP per capita could be indicitive of the health of the infrastructure where the earthquake occured. And the infrastructure in the location of the earthquake should have a major affect on the impact of the earthquake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression on Magnitude and GDP Per Capita\n",
    "r_squared2 = []\n",
    "\n",
    "intensity_regress2 = sm.ols(formula = 'Intensity ~ Magnitude + GDPPerCapita', data = earthquake_data).fit()\n",
    "r_squared2.append(intensity_regress2.rsquared)\n",
    "\n",
    "fatalities_regress2 = sm.ols(formula = 'Fatalities ~ Magnitude + GDPPerCapita', data = earthquake_data).fit()\n",
    "r_squared2.append(fatalities_regress2.rsquared)\n",
    "\n",
    "injuries_regress2 = sm.ols(formula = 'Injuries ~ Magnitude + GDPPerCapita', data = earthquake_data).fit()\n",
    "r_squared2.append(injuries_regress2.rsquared)\n",
    "\n",
    "displaced_regress2 = sm.ols(formula = 'displaced ~ Magnitude + GDPPerCapita', data = earthquake_data).fit()\n",
    "r_squared2.append(displaced_regress2.rsquared)\n",
    "\n",
    "impact_value_regress2 = sm.ols(formula = 'Impact ~ Magnitude + GDPPerCapita', data = earthquake_data).fit()\n",
    "r_squared2.append(impact_value_regress2.rsquared)\n",
    "\n",
    "damaged_buildings_regress2 = sm.ols(formula = 'Damaged ~ Magnitude + GDPPerCapita', data = earthquake_data).fit()\n",
    "r_squared2.append(damaged_buildings_regress2.rsquared)\n",
    "\n",
    "destroyed_buildings_regress2 = sm.ols(formula = 'Destroyed ~ Magnitude + GDPPerCapita', data = earthquake_data).fit()\n",
    "r_squared2.append(destroyed_buildings_regress2.rsquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R-Squared Value for Regressions Done on Magnitude and GDP Per Capita:\\n\")\n",
    "for i in range(len(impact_factors)):\n",
    "    print(\"The R-Squared Value for \" + impact_factors[i] + \" is \" + str(round(r_squared2[i], 3)) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression on Magnitude and GDP\n",
    "r_squared3 = []\n",
    "\n",
    "intensity_regress3 = sm.ols(formula = 'Intensity ~ Magnitude + GDP', data = earthquake_data).fit()\n",
    "r_squared3.append(intensity_regress3.rsquared)\n",
    "\n",
    "fatalities_regress3 = sm.ols(formula = 'Fatalities ~ Magnitude + GDP', data = earthquake_data).fit()\n",
    "r_squared3.append(fatalities_regress3.rsquared)\n",
    "\n",
    "injuries_regress3 = sm.ols(formula = 'Injuries ~ Magnitude + GDP', data = earthquake_data).fit()\n",
    "r_squared3.append(injuries_regress3.rsquared)\n",
    "\n",
    "displaced_regress3 = sm.ols(formula = 'displaced ~ Magnitude + GDP', data = earthquake_data).fit()\n",
    "r_squared3.append(displaced_regress3.rsquared)\n",
    "\n",
    "impact_value_regress3 = sm.ols(formula = 'Impact ~ Magnitude + GDP', data = earthquake_data).fit()\n",
    "r_squared3.append(impact_value_regress3.rsquared)\n",
    "\n",
    "damaged_buildings_regress3 = sm.ols(formula = 'Damaged ~ Magnitude + GDP', data = earthquake_data).fit()\n",
    "r_squared3.append(damaged_buildings_regress3.rsquared)\n",
    "\n",
    "destroyed_buildings_regress3 = sm.ols(formula = 'Destroyed ~ Magnitude + GDP', data = earthquake_data).fit()\n",
    "r_squared3.append(destroyed_buildings_regress3.rsquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R-Squared Value for Regressions Done on Magnitude and GDP:\\n\")\n",
    "for i in range(len(impact_factors)):\n",
    "    print(\"The R-Squared Value for \" + impact_factors[i] + \" is \" + str(round(r_squared3[i], 3)) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression on Magnitude and Population\n",
    "r_squared4 = []\n",
    "\n",
    "intensity_regress4 = sm.ols(formula = 'Intensity ~ Magnitude + Population', data = earthquake_data).fit()\n",
    "r_squared4.append(intensity_regress4.rsquared)\n",
    "\n",
    "fatalities_regress4 = sm.ols(formula = 'Fatalities ~ Magnitude + Population', data = earthquake_data).fit()\n",
    "r_squared4.append(fatalities_regress4.rsquared)\n",
    "\n",
    "injuries_regress4 = sm.ols(formula = 'Injuries ~ Magnitude + Population', data = earthquake_data).fit()\n",
    "r_squared4.append(injuries_regress4.rsquared)\n",
    "\n",
    "displaced_regress4 = sm.ols(formula = 'displaced ~ Magnitude + Population', data = earthquake_data).fit()\n",
    "r_squared4.append(displaced_regress4.rsquared)\n",
    "\n",
    "impact_value_regress4 = sm.ols(formula = 'Impact ~ Magnitude + Population', data = earthquake_data).fit()\n",
    "r_squared4.append(impact_value_regress4.rsquared)\n",
    "\n",
    "damaged_buildings_regress4 = sm.ols(formula = 'Damaged ~ Magnitude + Population', data = earthquake_data).fit()\n",
    "r_squared4.append(damaged_buildings_regress4.rsquared)\n",
    "\n",
    "destroyed_buildings_regress4 = sm.ols(formula = 'Destroyed ~ Magnitude + Population', data = earthquake_data).fit()\n",
    "r_squared4.append(destroyed_buildings_regress4.rsquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R-Squared Value for Regressions Done on Magnitude and Population:\\n\")\n",
    "for i in range(len(impact_factors)):\n",
    "    print(\"The R-Squared Value for \" + impact_factors[i] + \" is \" + str(round(r_squared4[i], 3)) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of those regressions were much on an improvement over the regression done with just magnitude. Now we will try some regressions using all the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression on Magnitude, GDP, Population, and GDP Per Capita\n",
    "r_squared5 = []\n",
    "\n",
    "intensity_regress5 = sm.ols(formula = 'Intensity ~ Magnitude + GDP + Population + GDPPerCapita', data = earthquake_data).fit()\n",
    "r_squared5.append(intensity_regress5.rsquared)\n",
    "\n",
    "fatalities_regress5 = sm.ols(formula = 'Fatalities ~ Magnitude + GDP + Population + GDPPerCapita', data = earthquake_data).fit()\n",
    "r_squared5.append(fatalities_regress5.rsquared)\n",
    "\n",
    "injuries_regress5 = sm.ols(formula = 'Injuries ~ Magnitude + GDP + Population + GDPPerCapita', data = earthquake_data).fit()\n",
    "r_squared5.append(injuries_regress5.rsquared)\n",
    "\n",
    "displaced_regress5 = sm.ols(formula = 'displaced ~ Magnitude + GDP + Population + GDPPerCapita', data = earthquake_data).fit()\n",
    "r_squared5.append(displaced_regress5.rsquared)\n",
    "\n",
    "impact_value_regress5 = sm.ols(formula = 'Impact ~ Magnitude + GDP + Population + GDPPerCapita', data = earthquake_data).fit()\n",
    "r_squared5.append(impact_value_regress5.rsquared)\n",
    "\n",
    "damaged_buildings_regress5 = sm.ols(formula = 'Damaged ~ Magnitude + GDP + Population + GDPPerCapita', data = earthquake_data).fit()\n",
    "r_squared5.append(damaged_buildings_regress5.rsquared)\n",
    "\n",
    "destroyed_buildings_regress5 = sm.ols(formula = 'Destroyed ~ Magnitude + GDP + Population + GDPPerCapita', data = earthquake_data).fit()\n",
    "r_squared5.append(destroyed_buildings_regress5.rsquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R-Squared Value for Regressions Done on Magnitude, GDP, Population, and GDP Per Capita:\\n\")\n",
    "for i in range(len(impact_factors)):\n",
    "    print(\"The R-Squared Value for \" + impact_factors[i] + \" is \" + str(round(r_squared5[i], 3)) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame to display regresssion data\n",
    "regression_df1 = pd.DataFrame(data = {\"Magnitude\": r_squared1, \"Magnitude and GDP Per Capita\": r_squared2, \"Magnitude and GDP\": r_squared3, \"Magnitude and Population\": r_squared4, \"Magnitude, GDP, Population, and GDP Per Capita\": r_squared5}, index = impact_factors)\n",
    "regression_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Data Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we start the cluster to find out what patterns can be seen and if we can find something use full. First we start with the principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat=Database[['Magnitude','Intensity (MMI / JMA)','Fatalities','Injuries','displaced','Impact value (D)','buildings damaged','buildings destroyed']].dropna()\n",
    "sca=scale(dat)\n",
    "pca_model = PCA()\n",
    "X_PCA = pca_model.fit_transform(sca)\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(X_PCA[:,0].min()-1,X_PCA[:,0].max()+1)\n",
    "ax.set_ylim(X_PCA[:,1].min()-1,X_PCA[:,1].max()+1)\n",
    "plt.scatter(X_PCA[:,0],X_PCA[:,1])\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Principal Component of States')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we see if which prnicpal components to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_ratio = pca_model.explained_variance_ratio_\n",
    "print(var_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter from excel sheets\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap([\"#e41a1c\",\"#984ea3\",\"#a65628\",\"#377eb8\"])\n",
    "plt.scatter(Database['Long'],Database['Lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter from excel sheets\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap([\"#e41a1c\",\"#984ea3\",\"#a65628\",\"#377eb8\"])\n",
    "plt.scatter(Database['Long'],Database['Lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kmeans from USGS data. We will use k-means for all other data\n",
    "quac=Current[['latitude','longitude']]\n",
    "scal=scale(quac)\n",
    "y_pred = KMeans(n_clusters=7,n_init=50).fit_predict(scal)\n",
    "quac['grouping']=y_pred\n",
    "plt.scatter(quac['longitude'],quac['latitude'],c=y_pred,cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Means is the best\n",
    "y_pred = KMeans(n_clusters=4).fit_predict(sca)\n",
    "dat['grouping']=y_pred\n",
    "plt.scatter(X_PCA[:,0],X_PCA[:,1],c=y_pred,cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_cluster_model = AgglomerativeClustering(linkage='complete', affinity='euclidean', n_clusters=7)\n",
    "y_pred1 = agg_cluster_model.fit_predict(sca)\n",
    "dat['grouping1']=y_pred1\n",
    "plt.scatter(X_PCA[:,0],X_PCA[:,1],c=y_pred1,cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3: Regressions on Clustered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Validation [Check Off: Katie, Paul]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probability analysis of earthquakes to be expanded for USGS map\n",
    "def probability(c):\n",
    "    its=[2,3,4,5,6,7,8,9,10]\n",
    "    m=[]\n",
    "    for i in range(len(its)-1):\n",
    "        stamp=earthquake_data[(its[i]<= earthquake_data['Magnitude'])&(earthquake_data['Magnitude']<its[i+1])]\n",
    "        try:\n",
    "            t=len(stamp.groupby(['Country']).get_group(c))/len(earthquake_data.groupby(['Country']).get_group(c))*100\n",
    "        except:\n",
    "            t='nana'\n",
    "        m.append(t)\n",
    "    return(m)\n",
    "\n",
    "probability('United Kingdom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions [Check Off: Katie, Paul]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
